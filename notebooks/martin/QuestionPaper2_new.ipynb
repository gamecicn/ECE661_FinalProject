{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "QuestionPaper2_new.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yom2joyA7VQb",
        "outputId": "ca9983c4-b272-47d4-b66a-7107a63a82a9"
      },
      "source": [
        "# set working dictionary\n",
        "import os\n",
        "import sys\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "os.chdir('/content/drive/MyDrive/ECE661/FinalProject')\n",
        "sys.path.append('/content/drive/MyDrive/ECE661/FinalProject')\n",
        "\n",
        "\n",
        "##################################\n",
        "\n",
        "# import necessary dependencies\n",
        "import argparse\n",
        "import os, sys\n",
        "import time\n",
        "from datetime import datetime \n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torchvision\n",
        "from torchvision import datasets, transforms\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"device:\", DEVICE)\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1sztfUlrFtjN"
      },
      "source": [
        "# Download Data - Cifar10"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7R8Oe5q7EGQ9",
        "outputId": "93025873-ef34-4603-e852-5c0e1ee471d6"
      },
      "source": [
        " \n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    datasets.CIFAR10(root='./data', train=True, transform=transforms.Compose([\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "    ]), download=True),\n",
        "    batch_size=128, shuffle=True)\n",
        "\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    datasets.CIFAR10(root='./data', train=False, transform=transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
        "    ])),\n",
        "    batch_size=100, shuffle=False)\n",
        "\n",
        "def get_accuracy(model, data_loader, device):\n",
        "    '''\n",
        "    Function for computing the accuracy of the predictions over the entire data_loader\n",
        "    '''\n",
        "    \n",
        "    correct_pred = 0 \n",
        "    n = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        for X, y_true in data_loader:\n",
        "\n",
        "            X = X.to(device)\n",
        "            y_true = y_true.to(device)\n",
        "\n",
        "            y_prob = model(X)\n",
        "            predicted_labels = torch.argmax(y_prob, 1)\n",
        "\n",
        "            n += y_true.size(0)\n",
        "            correct_pred += (predicted_labels == y_true).sum()\n",
        "\n",
        "    return correct_pred.float() / n\n",
        "\n",
        "def plot_losses(train_losses, valid_losses):\n",
        "    '''\n",
        "    Function for plotting training and validation losses\n",
        "    '''\n",
        "    \n",
        "    # temporarily change the style of the plots to seaborn \n",
        "    plt.style.use('seaborn')\n",
        "\n",
        "    train_losses = np.array(train_losses) \n",
        "    valid_losses = np.array(valid_losses)\n",
        "\n",
        "    fig, ax = plt.subplots(figsize = (8, 4.5))\n",
        "\n",
        "    ax.plot(train_losses, color='blue', label='Training loss') \n",
        "    ax.plot(valid_losses, color='red', label='Validation loss')\n",
        "    ax.set(title=\"Loss over epochs\", \n",
        "            xlabel='Epoch',\n",
        "            ylabel='Loss') \n",
        "    ax.legend()\n",
        "    fig.show()\n",
        "    \n",
        "    # change the plot style to default\n",
        "    plt.style.use('default')\n",
        "\n",
        "def train(train_loader, model, criterion, optimizer, device):\n",
        "    '''\n",
        "    Function for the training step of the training loop\n",
        "    '''\n",
        "    model.train()\n",
        "    running_loss = 0\n",
        "    \n",
        "    for X, y_true in train_loader:\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        X = X.to(device)\n",
        "        y_true = y_true.to(device)\n",
        "    \n",
        "        # Forward pass\n",
        "        y_hat = model(X) \n",
        "        loss = criterion(y_hat, y_true) \n",
        "        running_loss += loss.item() * X.size(0)\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "    epoch_loss = running_loss / len(train_loader.dataset)\n",
        "    return model, optimizer, epoch_loss\n",
        "\n",
        "def validate(valid_loader, model, criterion, device):\n",
        "    '''\n",
        "    Function for the validation step of the training loop\n",
        "    '''\n",
        "   \n",
        "    model.eval()\n",
        "    running_loss = 0\n",
        "    \n",
        "    for X, y_true in valid_loader:\n",
        "    \n",
        "        X = X.to(device)\n",
        "        y_true = y_true.to(device)\n",
        "\n",
        "        # Forward pass and record loss\n",
        "        y_hat = model(X) \n",
        "        loss = criterion(y_hat, y_true) \n",
        "        running_loss += loss.item() * X.size(0)\n",
        "\n",
        "    epoch_loss = running_loss / len(valid_loader.dataset)\n",
        "        \n",
        "    return model, epoch_loss\n",
        "\n",
        "\n",
        "def training_loop(model, criterion, optimizer, train_loader, valid_loader, epochs, device, scheduler, print_every=1):\n",
        "    '''\n",
        "    Function defining the entire training loop\n",
        "    '''\n",
        "    \n",
        "    # set objects for storing metrics\n",
        "    best_loss = 1e10\n",
        "    train_losses = []\n",
        "    valid_losses = []\n",
        " \n",
        "    # Train model\n",
        "    for epoch in range(0, epochs):\n",
        "\n",
        "        # training\n",
        "        model, optimizer, train_loss = train(train_loader, model, criterion, optimizer, device)\n",
        "        train_losses.append(train_loss)\n",
        "\n",
        "        # validation\n",
        "        with torch.no_grad():\n",
        "            model, valid_loss = validate(valid_loader, model, criterion, device)\n",
        "            valid_losses.append(valid_loss)\n",
        "\n",
        "        if epoch % print_every == (print_every - 1):\n",
        "            \n",
        "            train_acc = get_accuracy(model, train_loader, device=device)\n",
        "            valid_acc = get_accuracy(model, valid_loader, device=device)\n",
        "                \n",
        "            print(f'{datetime.now().time().replace(microsecond=0)} --- '\n",
        "                  f'Epoch: {epoch}\\t'\n",
        "                  f'Train loss: {train_loss:.4f}\\t'\n",
        "                  f'Valid loss: {valid_loss:.4f}\\t'\n",
        "                  f'Train accuracy: {100 * train_acc:.2f}\\t'\n",
        "                  f'Valid accuracy: {100 * valid_acc:.2f}')\n",
        "            \n",
        "        scheduler.step()\n",
        "\n",
        "    plot_losses(train_losses, valid_losses)\n",
        "    \n",
        "    return model, optimizer, (train_losses, valid_losses)\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A5Hc8PZyGVaG"
      },
      "source": [
        "# Model Define\n",
        "\n",
        "- Teacher Model ： ResNet18\n",
        "- Student Model ： VGG13"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JA_dE6NQGUF4"
      },
      "source": [
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    expansion = 1\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(\n",
        "            in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.bn2(self.conv2(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    expansion = 4\n",
        "\n",
        "    def __init__(self, in_planes, planes, stride=1):\n",
        "        super(Bottleneck, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3,\n",
        "                               stride=stride, padding=1, bias=False)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.conv3 = nn.Conv2d(planes, self.expansion *\n",
        "                               planes, kernel_size=1, bias=False)\n",
        "        self.bn3 = nn.BatchNorm2d(self.expansion*planes)\n",
        "\n",
        "        self.shortcut = nn.Sequential()\n",
        "        if stride != 1 or in_planes != self.expansion*planes:\n",
        "            self.shortcut = nn.Sequential(\n",
        "                nn.Conv2d(in_planes, self.expansion*planes,\n",
        "                          kernel_size=1, stride=stride, bias=False),\n",
        "                nn.BatchNorm2d(self.expansion*planes)\n",
        "            )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = F.relu(self.bn2(self.conv2(out)))\n",
        "        out = self.bn3(self.conv3(out))\n",
        "        out += self.shortcut(x)\n",
        "        out = F.relu(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class ResNet(nn.Module):\n",
        "    def __init__(self, block, num_blocks, num_classes=10):\n",
        "        super(ResNet, self).__init__()\n",
        "        self.in_planes = 64\n",
        "\n",
        "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3,\n",
        "                               stride=1, padding=1, bias=False)\n",
        "        self.bn1 = nn.BatchNorm2d(64)\n",
        "        self.layer1 = self._make_layer(block, 64, num_blocks[0], stride=1)\n",
        "        self.layer2 = self._make_layer(block, 128, num_blocks[1], stride=2)\n",
        "        self.layer3 = self._make_layer(block, 256, num_blocks[2], stride=2)\n",
        "        self.layer4 = self._make_layer(block, 512, num_blocks[3], stride=2)\n",
        "        self.linear = nn.Linear(512*block.expansion, num_classes)\n",
        "\n",
        "    def _make_layer(self, block, planes, num_blocks, stride):\n",
        "        strides = [stride] + [1]*(num_blocks-1)\n",
        "        layers = []\n",
        "        for stride in strides:\n",
        "            layers.append(block(self.in_planes, planes, stride))\n",
        "            self.in_planes = planes * block.expansion\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = F.relu(self.bn1(self.conv1(x)))\n",
        "        out = self.layer1(out)\n",
        "        out = self.layer2(out)\n",
        "        out = self.layer3(out)\n",
        "        out = self.layer4(out)\n",
        "        out = F.avg_pool2d(out, 4)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.linear(out)\n",
        "        return out\n",
        "\n",
        "cfg = {\n",
        "    'VGG11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
        "    'VGG13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
        "    'VGG16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
        "    'VGG19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n",
        "}\n",
        "\n",
        "\n",
        "class VGG(nn.Module):\n",
        "    def __init__(self, vgg_name):\n",
        "        super(VGG, self).__init__()\n",
        "        self.features = self._make_layers(cfg[vgg_name])\n",
        "        self.classifier = nn.Linear(512, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.features(x)\n",
        "        out = out.view(out.size(0), -1)\n",
        "        out = self.classifier(out)\n",
        "        return out\n",
        "\n",
        "    def _make_layers(self, cfg):\n",
        "        layers = []\n",
        "        in_channels = 3\n",
        "        for x in cfg:\n",
        "            if x == 'M':\n",
        "                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
        "            else:\n",
        "                layers += [nn.Conv2d(in_channels, x, kernel_size=3, padding=1),\n",
        "                           nn.BatchNorm2d(x),\n",
        "                           nn.ReLU(inplace=True)]\n",
        "                in_channels = x\n",
        "        layers += [nn.AvgPool2d(kernel_size=1, stride=1)]\n",
        "        return nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "def ResNet18():\n",
        "    return ResNet(BasicBlock, [2, 2, 2, 2]).to(DEVICE)\n",
        "\n",
        "def VGG13():\n",
        "    return VGG('VGG13').to(DEVICE)\n"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BeD7p9aWPFVq"
      },
      "source": [
        "# Train Teacher model on ground truth"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dY-3kmgKF8Wk",
        "outputId": "9f07fb92-6c29-431b-f100-dc4be2a4f892"
      },
      "source": [
        "import os.path\n",
        "\n",
        "RANDOM_SEED = 1234\n",
        "RES18_CP = \"ResNet18_new_ground_truth.pt\"\n",
        "\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "\n",
        "model_student = ResNet18()\n",
        "\n",
        "if os.path.exists(RES18_CP):\n",
        "\n",
        "  # load model Directly\n",
        "  print(\"==========> Load Parameters\")\n",
        "  model_student.load_state_dict(torch.load(RES18_CP))\n",
        "  \n",
        "else:\n",
        "\n",
        "  # If model not trained \n",
        "  print(\"==========> Not find checkpoint, train\")\n",
        "\n",
        "  N_EPOCHS = 40\n",
        "\n",
        "  criterion = nn.CrossEntropyLoss()\n",
        "  optimizer = optim.SGD(model_student.parameters(), lr=0.01,\n",
        "               momentum=0.9, weight_decay=5e-4)\n",
        "  scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n",
        "\n",
        "  model_student, optimizer, _ = training_loop(model_student, criterion, optimizer, train_loader, val_loader, N_EPOCHS, DEVICE, scheduler)\n",
        "\n",
        "  # Save model \n",
        "  torch.save(model_student.state_dict(), RES18_CP)\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==========> Load Parameters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RkUOmWHEcP8x"
      },
      "source": [
        "# Train teacher on student\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b8BQFeDMlHW8"
      },
      "source": [
        "def plot_losses_soft(train_losses, valid_losses, title_prefix):\n",
        "    '''\n",
        "    Function for plotting training and validation losses\n",
        "    '''\n",
        "    plt.figure(figsize=(6, 4),dpi=100)\n",
        "    train_losses = np.array(train_losses) \n",
        "    valid_losses = np.array(valid_losses)\n",
        "    \n",
        "    plt.plot(train_losses, color='blue', label='Training loss') \n",
        "    plt.plot(valid_losses, color='red', label='Validation loss')\n",
        "    plt.title(f\"{title_prefix}: Loss over epochs\") \n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss') \n",
        "    plt.legend()\n",
        "    \n",
        "    plt.show()\n",
        "\n",
        "def train_soft(train_loader, model, criterion, optimizer, device, teacher_model=None):\n",
        "    '''\n",
        "    Function for the training step of the training loop\n",
        "    '''\n",
        "    model.train()\n",
        "    running_loss = 0\n",
        "    \n",
        "    for X, y_true in train_loader:\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        X = X.to(device)\n",
        "        y_true = y_true.to(device)\n",
        "    \n",
        "        # Forward pass\n",
        "        y_hat = model(X) \n",
        "        if not teacher_model:\n",
        "            loss = criterion(y_hat, y_true)\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "                y_teacher = teacher_model(X)\n",
        "            loss = criterion(y_hat, y_true, y_teacher)\n",
        "            \n",
        "        running_loss += loss.item() * X.size(0)\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "    epoch_loss = running_loss / len(train_loader.dataset)\n",
        "    return model, optimizer, epoch_loss\n",
        "\n",
        "\n",
        "def training_loop_soft(model, train_criterion, valid_criterion, scheduler, optimizer, train_loader, valid_loader, \n",
        "                  epochs, device, params, print_every=1, teacher_model=None):\n",
        "    '''\n",
        "    Function defining the entire training loop\n",
        "    '''\n",
        "    # set objects for storing metrics\n",
        "    best_loss = 1e10\n",
        "    train_losses = []\n",
        "    valid_losses = []\n",
        "    acc_epochs = []\n",
        "    train_accus = []\n",
        "    valid_accus = []\n",
        "    \n",
        "\n",
        "    # Train model\n",
        "    for epoch in range(0, epochs):\n",
        "\n",
        "        # training\n",
        "        model, optimizer, train_loss = train_soft(train_loader, model, train_criterion, optimizer, device, teacher_model)\n",
        "        train_losses.append(train_loss)\n",
        "\n",
        "        # validation\n",
        "        with torch.no_grad():\n",
        "            model, valid_loss = validate(valid_loader, model, valid_criterion, device)\n",
        "            valid_losses.append(valid_loss)\n",
        "\n",
        "        if epoch % print_every == (print_every - 1):\n",
        "            acc_epochs.append(epoch)\n",
        "            train_acc = get_accuracy(model, train_loader, device=device)\n",
        "            train_accus.append(train_acc)\n",
        "            valid_acc = get_accuracy(model, valid_loader, device=device)\n",
        "            valid_accus.append(valid_acc)\n",
        "                \n",
        "            print(f'{datetime.now().time().replace(microsecond=0)} --- '\n",
        "                  f'Epoch: {epoch}\\t'\n",
        "                  f'Train loss: {train_loss:.4f}\\t'\n",
        "                  f'Valid loss: {valid_loss:.4f}\\t'\n",
        "                  f'Train accuracy: {100 * train_acc:.2f}\\t'\n",
        "                  f'Valid accuracy: {100 * valid_acc:.2f}')\n",
        "            \n",
        "        scheduler.step()\n",
        "\n",
        "    # save metrics\n",
        "    plot_losses_soft(train_losses, valid_losses, params['title'])\n",
        "\n",
        "    '''\n",
        "    res_acc = pd.DataFrame({\n",
        "        \"acc_epoch\": acc_epochs,\n",
        "        \"train_accus\": train_accus,\n",
        "        \"valid_accus\": valid_accus\n",
        "    })\n",
        "    res_loss = pd.DataFrame({\n",
        "        \"loss_epoch\": range(0, epochs),\n",
        "        \"train_losses\": train_losses,\n",
        "        \"valid_losses\": valid_losses,\n",
        "    })\n",
        "    '''\n",
        "    str_time = datetime.now().strftime(\"%m-%d_%H-%M\")\n",
        "    #res_acc.to_csv(f\"{params['dir']}/{params['prefix']}_{str_time}_accus.csv\", index=False)\n",
        "    #res_loss.to_csv(f\"{params['dir']}/{params['prefix']}_{str_time}_losses.csv\", index=False)\n",
        "    \n",
        "    return model, optimizer, (train_losses, valid_losses)"
      ],
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KgzGC0-yioMo",
        "outputId": "154847ed-b89d-415b-9666-5b6d83196e30"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "class SoftCrossEntropyLoss(nn.Module):\n",
        "    def __init__(self, temperature, alpha):\n",
        "        super(SoftCrossEntropyLoss, self).__init__()\n",
        "        self.T = temperature\n",
        "        self.alpha = alpha\n",
        "    \n",
        "    def forward(self, student_pred, label, teacher_pred):\n",
        "        return nn.KLDivLoss(reduction=\"batchmean\")(F.log_softmax(student_pred/self.T,dim=1), F.softmax(teacher_pred/self.T, dim=1)) * self.alpha * self.T * self.T + \\\n",
        "    F.cross_entropy(student_pred, label) * (1-self.alpha)\n",
        "\n",
        "\n",
        "# Train\n",
        "temperatures = [2]\n",
        "LEARNING_RATE = 0.001\n",
        "N_EPOCHS = 40\n",
        "ALPHA = 0.95\n",
        "\n",
        "model_student = ResNet18()\n",
        "\n",
        "VGG13_CP = \"VGG13_cifar_ground_truth.pt\"\n",
        "model_teacher = VGG13()\n",
        "model_teacher.load_state_dict(torch.load(VGG13_CP))\n",
        "\n",
        "for t in temperatures:\n",
        "\n",
        "    print(f\"=======> Temperature: {t}\")\n",
        " \n",
        "    optimizer = optim.SGD(model_student.parameters(), lr=0.01,\n",
        "               momentum=0.9, weight_decay=5e-4)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n",
        "\n",
        "    valid_criterion = nn.CrossEntropyLoss()\n",
        "    train_criterion = SoftCrossEntropyLoss(t, ALPHA)\n",
        "\n",
        "    prefix = f\"student_T{t}_α{ALPHA}\"\n",
        "    dir_path = f\"./student_models/{prefix}\"\n",
        "\n",
        "    #if not os.path.exists(dir_path):\n",
        "    #    os.makedirs(dir_path, exist_ok=True)\n",
        "    \n",
        "    params = {\"dir\": dir_path,\n",
        "          \"prefix\": prefix, \n",
        "          \"title\": f\"Student T{t} α{ALPHA}\"\n",
        "         }\n",
        "\n",
        "    model_student, optimizer, _ = training_loop_soft(model_student, train_criterion, valid_criterion, scheduler, optimizer, train_loader, val_loader,\n",
        "                                        N_EPOCHS, DEVICE, params, teacher_model=model_teacher)\n",
        "    \n",
        "    torch.save(model_student.state_dict(), f\"ResNet18_learn_from_Vgg13_t{t}.pt\")   "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=======> Temperature: 2\n"
          ]
        }
      ]
    }
  ]
}