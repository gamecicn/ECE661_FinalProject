# Deep Neural Network Knowledge Distillation

**Knowledge distillation** is a **model compression** method with the goal of deploying  deep networks in low computation required and storage limited devices without significant decrease in accuracy.  In this repository, we implement three different model distillation methods: normal KD, reverse KD and self KD. 



Firstly, we checked the performance of distilling smaller linear models (student) from larger  linear models (teacher) (**Normal KD**).

- Compare performance and efficiency of the larger model trained on true labels with that  of the smaller model.
- Train a smaller model on a combination of true labels and soft labels from teacher model, then compare it with teacher model as well as the small model trained only on true labels.
- Experiment with adding temperature and the effect of different temperatures on the performance of distilled student model.
- Use distilled modelsâ€˜ prediction on the omitted digit to evaluate the generalization ability learned from teacher model.

Then we let large model (teacher) learn from small model (student) (**Reversed KD**).

- Train the large model on a combination of true labels and soft labels from the small model. Then compare the accuracy of the large model learned from the small model with a large model learned from the ground truth.

At last, we let models learn from themselves (**Self KD**).

- Train a student model on a combination of true and soft labels from a teacher model. The  teacher model shares the same network structure as the student model. Then compare performance between teacher and student model.



![Structure](./image/three_model.png)



## Run the notebooks

The notebooks are named in the questions order and stored in ./notebooks folder.   



#### Question 1 
*Design a small network and a large network by yourself following the instructions on section 3 in [1]. Report the FLOPS and number of parameters of both models then train them on MNIST and report their performance (accuracy) on validation dataset.*

**Notebook:**  question_1_cumbersome_small_model.ipynb



#### Question 2 
*Use the soft labels generated by the larger model to train the small network on MNIST and compare the performance with the same small model trained with ground truth.*

**Notebook:**  question_2_distils_outperforms.ipynb



#### Question 3 
*Try different temperature T in equation (1) in [1] and analyze the influence of this parameter on distillation.*

**Notebook:**  



#### Question 4 
*We refer to the dataset which the large model extracts soft label of, and the small model is trained on as transfer dataset. Obviously, the transfer dataset need not be identical to the training set of the large model. Now try to randomly omit one digit from MNIST as transfer set and repeat the distillation process (large model is still trained on full MNIST). Test the trained small model on complete validation dataset to see what the performance of this small model on the digit that it never sees during training.*

**Notebook:**  



#### Question 5
*Then, try to reverse the teacher and the student model [2], which means now the student model is larger than teacher model. For this question, you could pick up two existing models by yourself as teacher and student models, such as ResNet34 and ResNet50 and experiment on Cifar10 dataset.*

**Notebook:**  question_5_6_reverse_and_self_KD.ipynb

The question 5 was answered in section **Reverse KD** .  Please make sure run the following sections at first.

- Import necessary dependencies
- Download Data - CIFAR10
- Model Define
- KD Utility Functions



#### Question 6 
*Instead of learning from a teacher model, model can also learn from itself for improving performance, which is known as self distillation. Implement self distillation for ResNet50 on Cifar10 based on section 3 of [2]. Then compare the performance between traditional training model and model trained with self distillation.*

**Notebook:**  question_5_6_reverse_and_self_KD.ipynb

The question 6 was answered in section **Self KD ** .  Please make sure run the following sections at first.

- Import necessary dependencies
- Download Data - CIFAR10
- Model Define
- KD Utility Functions

## Checkpoints

If you don't want to run the code from scratch, you can download the checkpoints in the ./checkpoints  of the models. 

## References:

[1] Hinton, Geoffrey, Oriol Vinyals, and Jeff Dean. "Distilling the knowledge in a neural network." arXiv 
preprint arXiv:1503.02531 (2015).

[2] Yuan, Li, et al. "Revisit knowledge distillation: a teacher-free framework." (2019).

